\section{Resultados}
En esta sección incluiremos los resultados de la experimentación que realizamos con el perceptrón desarrollado.\\
La idea general de los experimentos es intentar medir la influencia de alguna(s) variable(s) específicas sobre la performance
de la red. Para ello intentamos fijar todas las demás variables en valores óptimos y poner a prueba las que nos interesaban.\\

A medida que fuimos encontrando valores óptimos para las distintas variables, los fuimos utilizando para el resto de los experimentos subsiguientes.\\

En el primer experimento, intentamos determinar el mejor número de capas ocultas para utilizar en el ejercicio 1. A continuación, los resultados.

\subsection{Ejercicio 1}

\subsubsection{Variación del número de capas ocultas.}
La idea de este experimento es determinar la cantidad de capas ocultas óptimas para el ejercicio 1. Para ello fijamos las demás variables con esta configuración:

\begin{itemize}
\item Épocas: 250
\item ETA: 0.05
\item 10 neuronas por capa.
\item 70 \% del dataset como training, 20 \% training, 10\% validación.
\item Distribución de Pesos: Normal
\item Entrenamiento Estocástico
\item Sin Momentum
\item Sin Early Stopping
\end{itemize}

Esta configuración es arbitraria y está basada en pruebas informales que realizó el equipo antes de comenzar la experimentación formal. A medida que avanzaron los experimentos y fuimos descubriendo mejores valores, los fuimos actualizando para las siguientes pruebas.\\

El experimento fue desarrollado así: 
\begin{itemize}
\item Dividimos el dataset en datos de entrenamiento, validación y testing. Utilizamos la misma división para todas las ejecuciones.
\item Con la configuración mencionada, corrimos 8 rondas de cada número de capas ocultas utilizando: 1, 2, 3, 5 y 10 capas.
Es decir, primero se ejecutaron 8 veces el entrenamiento, validación y testing de una red con 1 capa. Luego repetimos con 2, con 3, etc.
\item Para cada cantidad de capas, promediamos los resultados de cada una de las 8 rondas. Con ello obtuvimos un error final (función de costo) de entrenamiento promedio , un error de validación promedio y una efectividad de testing promedio. Llamamos efectividad de testing a la cantidad de resultados del dataset de testing correctamente predichos.
\item El error final de cada ronda corresponde a la función de costo de dicha corrida dividido la cantidad de patrones procesados.
\end{itemize}

Observemos los resultados:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/fig1_cant_capas_error_final.png}
  \caption{Error final (función de costo) promedio para cada cantidad de neuronas utilizando el dataset de entrenamiento}
  \end{center}
\end{figure}

Podemos ver en la figura 1 que los mejores errores fueron obtenidos con 2 y 3 capas. Estos datos pertenecen al error final promediado, usando el dataset de entrenamiento.\\
Notamos también que con más de 3 capas ya los números se empiezan a distorsionar y perder mucha precisión.

Para el error final en los datos de validación, obtuvimos los siguientes resultados:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/fig2_cant_capas_error_valid.png}
  \caption{Error (función de costo) promedio para cada cantidad de neuronas utilizando el dataset de validación}
  \end{center}
\end{figure}


En la figura 5 podemos notar que el valor de error con el dataset de validación también 
genera mejores números con 2 y 3 capas. Especialmente se aprecia un mejor valor para la arquitectura de 2 capas.\\
Observamos nuevamente que los valores más altos no rinden igual de bien que los anteriores.

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/fig3_cant_capas_testing.png}
  \caption{Tasa de predicciones correctas, en porcentaje, para el dataset de testing}
  \end{center}
\end{figure}

Para la efectividad en la predicción de los datos de testing, observamos que la red de 1 capa generó aciertos cercanos al 82\%, mientras que las de 2 y 3 capas superan el 90\%, teniendo la de 2 capas un desempeño levemente mejor.\\
Nos sorprendíó ver que las redes de 5 y 10 capas lograron una muy baja eficiencia, cercana al 50\%.\\

Teniendo en cuenta los resultados presentados en estas pruebas, nos decidimos por una arquitectura de 2 capas ocultas. Aunque se obtuvieron resultados casi igual de buenos con arquitectura de 3 capas, \textbf{la de 2 capas presenta mejor velocidad de ejecución y resultados levemente mejores}.\\

\newpage

\subsubsection{Variación del número de neuronas ocultas.}

Una vez obtenido el número óptimo de capas ocultas, quisimos determinar cual era la cantidad de nueronas que debía contener
cada una de estas capas. Para simplificar el problema, decidimos que todas las capas tuvieran la misma cantidad de neuronas.\\
De esta forma, elegimos las siguientes medidas: 2 capas de 2, 5, 7, 10, 15 y 20 neuronas. Estas medidas fueron elegidas para intentar representar una cantidad baja de neuronas como 2,5,7 y otras más altas, por arriba de 10. No consideramos cantidades mayores de neuronas ya que los resultados no mejoran ostensiblemente pasando las 20 neuronas, teniendo en cuenta la relativa
sencillez del ejercicio en cuestión. Además, arquitecturas con más de 20 neuronas por capa afectan notablemente la velocidad de ejecución de la red, relentizando las pruebas y haciéndolas engorrosas e innecesarias.\\

De forma similar al experimento anterior, esta prueba consistió en procesar el dataset completo 8 veces con cada cantidad de neuronas distintas, promediar los errores finales de entrenamiento y de validación junto con la eficiencia obtenida en los datos de testing.\\
Para el error final, obtuvimos los siguientes resultados:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/fig4_cant_neuro_error_final.png}
  \caption{Error final (función de costo) promedio para cada cantidad de neuronas utilizando el dataset de entrenamiento}
  \end{center}
\end{figure}

En la figura 4 se puede observar que el valor de la función de costo decrece a medida que la cantidad
de neuronas crece. Este efecto se vuelve mucho menos notable a partir de las 10 neuronas por capa.\\

Para el error final en los datos de validación, obtuvimos los siguientes resultados:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/fig5_cant_neuro_error_valid.png}
  \caption{Error (función de costo) promedio para cada cantidad de neuronas utilizando el dataset de validación}
  \end{center}
\end{figure}

En la figura 5 podemos notar que el valor de error con el dataset de validación también decrece a medida que sube la cantidad
de neuronas, aunque no de forma uniforme. Podemos apreciar que a partir de las 10 neuronas, el error se mantiene por debajo de 0.15.\\

Para la efectividad en la predicción de los datos de testing, observamos:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/fig6_cant_neuro_testing.png}
  \caption{Tasa de predicciones correctas, en porcentaje, para el dataset de testing}
  \end{center}
\end{figure}

Nuevamente podemos apreciar en la figura 6, que a medida que crece la cantidad de neuronas, crece la cantidad de aciertos sobre el conjunto de datos de test. Sin embargo, los resultados se comienzan a estancar a partir de las 10 neuronas en valores cercanos al 90 \% de aciertos.\\

Observando los resultados obtenidos en las pruebas, decidimos que la mejor configuración es con 10 neuronas por capa. Esta decisión se justifica teniendo en cuenta que los resultados no mejoran demasiado pasando de 10 y si empeora notablemente la velocidad de la red.\\
Por ende, \textbf{en vistas a los buenos resultados obtenidos con 10 neuronas y no obteniendo mejoras sustanciales con cantidades mayores}, nos quedaremos con este valor.

\subsubsection{Performance de la red, con entrenamiento sin momentum y con momentum.}

El momentum es una memoria o inercia que nos permite que los cambios en el vector de pesos $w$ sean suaves ya que incluyen información sobre el cambio de peso anterior. 
En el proceso de backpropagation de la red, cuando actualizamos los pesos, utilizaremos el momentum de la siguiente manera:
\\
\\
$\Delta w_{ij}^{m} = \eta \delta_{i}^{m}V_{j}^{m} + \alpha \Delta w_{pq}^{m-1}$
\\

Para este experimento, mantenemos la configuración que veniamos utilizando en experimentos anteriores con 2 capas de 10 neuronas. Probaremos el comportamiento de la red 
variando el momentum desde $0$ (o sin momentum) hasta un valor de momentum de \textbf{0.9}. No utilizaremos valores mayores de \textbf{0.9} ya que significaría que el peso anterior que 
tenía el eje se estaría acumulando con el nuevo peso en su completitud, lo cual nos parece una exageración en este caso. Dado que en posteriores experimentos realizaremos
pruebas variando el momentum y el learning rate en conjunto, para este experimento, mantendremos el learning rate de \textbf{0.05}.

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/momentum_0.png}
  \caption{Evolución del error de entrenamiento para momentum 0 (sin momentum)}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/momentum_0_05.png}
  \caption{Evolución del error de entrenamiento para momentum 0.05}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

A partir de los resultados podemos intuir que manteniendo el learning rate elegido, no usar momentum o usar momentum \textbf{0.05 o 0.5} da los mejores resultados. Claramente
usar \textbf{0.9} es casi como al nuevo peso de cada eje sumarle el peso anterior, por eso la oscilación tan pronunciada. Sin embargo, y para llegar a una conclusión del mejor 
valor de momentum, analizemos los resultados de la evolución del error para nuestro conjunto de validación:

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/momentum_0_1.png}
  \caption{Evolución del error de entrenamiento para momentum 0.1}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/momentum_0_5.png}
  \caption{Evolución del error de entrenamiento para momentum 0.5}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/momentum_0_9.png}
  \caption{Evolución del error de entrenamiento para momentum 0.9}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/momentum_promedios_entrenamiento.png}
  \caption{Evolución del error de entrenamiento promedio para todos los valores de momentum}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
  \begin{center}
  \includegraphics[scale=0.80]{graficos/momentum_promedios_validacion.png}
  \caption{Evolución del error de validación promedio para todos los valores de momentum}
  \end{center}
\end{figure}

Teniendo en cuenta ahora también nuestro error de validación final para todas las corridas y valores de momentum, podemos concluir entonces que manteniendo nuestro learning
rate fijo en \textbf{0.05}, lo mejor es no usar momentum.

\subsubsection{Performance de la red, con entrenamiento sin y con parámetros adaptativos.}

Utilizar parámetros adaptativos, como variar el learning rate, nos permiten volver a un estado anterior de la red y corregir el learning rate que usamos en 
ese momento para que esta vez nos de un mejor resultado. Para nuestra configuración adaptativa, si el error crece continuamente reducimos el learning a la mitad, 
mientras que si el error va en bajada, aumentamos el learning rate en un $10\%$. Manteniendo los parámetros de configuración que venimos utilizando, hicimos experimentos
sobre entrenamientos estocásticos, batch y mini batch. 

A partir de los resultados de entrenamiento estocástico, podemos observar que usar parámetros adaptativos no solo no ayuda al aprendizaje de la red, sino que
además lo perjudica en gran magnitud. Esto puede deberse principalmente a que la red esta continuamente ajustando sus pesos en cada entrada del dataset para
poder con el learning rate actual, mejorar la precisión y reducir el error. Si cambiamos el learning rate los pesos que ajustamos en cada paso dejan de tener
sentido y el entrenamiento termina dando malos resultados.

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_entrenamiento.png}
  \caption{Comparación del error en el conjunto de entrenamiento para estocástico (250 épocas)}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_validacion.png}
  \caption{Comparación del error en el conjunto de validación para estocástico (250 épocas)}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

Con respecto a batch, nos dimos cuenta que los parámetros adaptativos permiten que se obtengan muy buenos resultados. Incluso se puede ver que el error de 
validación llega a ser menor que en el entrenamiento estocástico. Sin embargo, tuvimos que utilizar el doble de épocas para conseguir los mismos. 

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_entrenamiento_batch_05.png}
  \caption{Comparación del error en el conjunto de entrenamiento para batch (250 épocas)}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_validacion_batch_05.png}
  \caption{Comparación del error en el conjunto de validación para batch (250 épocas)}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_entrenamiento_batch_500ep.png}
  \caption{Comparación del error en el conjunto de entrenamiento para batch (500 épocas)}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_validacion_batch_500ep.png}
  \caption{Comparación del error en el conjunto de validación para batch (500 épocas)}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

Para el entrenamiento mini batch, confirma la idea de que el learning rate adaptativo es útil solo en batch y no para estocástico. Realizamos un par de
pruebas y vimos que mientras más cerca de 1 (estocástico) estuviese el tamaño del batch, peor resultados da el adaptivo, mientras que si más cerca está del
tamaño total del dataset de entrenamiento, mejor funciona el learning rate adaptativo. 

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_entrenamiento_batch.png}
  \caption{Comparación del error en el conjunto de entrenamiento para mini batch de 10, cercano a ser estocástico (500 épocas)}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/adaptativos_promedios_validacion_batch.png}
  \caption{Comparación del error en el conjunto de validación para batch de 10, cercano a ser estocástico (500 épocas)}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

Nuestra conclusión para este experimento es que si bien se llegan a alcanzar buenos resultados con el entrenamiento batch, es necesario utilizar el doble de épocas
lo cual hace que tarde mucho más el entrenamiento. Online learning en la mitad de las épocas y sin learning rate adaptativo obtiene resultados similares.

\subsubsection{Performance de la red, variando simultáneamente el factor de aprendizaje $\mu$, y el parámetro $\alpha$ del momentum.}

Para este experimento, utilizamos un conjunto de diferentes valores de momentums para diferentes valores del learning rate de manera de poder encontrar
donde se comportaba de mejor manera la red. El conjunto de valores de momentums es \textbf{0, 0.05, 0.1, 0.5 y 0.9}, los cuales se probaron con los valores
de learning rate \textbf{0.1, 0.05, 0.01, 0.005, 0.001}.

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento_0.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.1}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion_0.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.1}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento_1.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.05}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion_1.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.05}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento_2.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.01}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion_2.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.01}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento_3.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.005}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion_3.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.005}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento_4.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.001}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion_4.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.001}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

A partir de los resultados podemos apreciar que los valores más bajos de error en entrenamiento y, aún más importante, en validación, se obtuvieron con
learning rate \textbf{0.1 y 0.05}. Es por eso que, siendo que cercanos a estos valores están nuestros mejores resultados, hicimos unas pruebas extras sobre
valores cercanos a los mismos. Esto es, probamos también con \textbf{0.08, 0.15 y 0.2} para intentar encontrar el mejor valor para nuestro learning rate en
entrenamiento estocástico.

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento__0.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.08}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion__0.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.08}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento__1.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.15}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion__1.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.15}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_entrenamiento__2.png}
  \caption{Evolución del error de entrenamiento con momentum variable y learning rate fijo 0.2}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{graficos/eta_x_momentum_promedios_validacion__2.png}
  \caption{Evolución del error de validación con momentum variable y learning rate fijo 0.2}
  \label{fig:sub2}
\end{subfigure}
\end{figure}

Como se puede observar, los mejores resultados siguen siendo usando un learning rate de \textbf{0.1 y de 0.05}. Más interesante todavía es que en 
anteriores experimentos con el momentum y un learning rate de \textbf{0.05} habíamos visto que el momentum en \textbf{0} era la mejor opción. Sin embargo,
 en estos nuevos resultados se puede ver que usar momentum \textbf{0.1} es mejor que \textbf{0} tanto para el learning rate de \textbf{0.1 y de 0.05}. 
 Esto puede deberse a como se armaron los conjuntos de entrenamiento y validación.

\subsubsection{Performance de la red, variando técnicas de entrenamiento (estocástico, batch y mini batch) y de inicialización de pesos}

En este experimento decidimos probar dos variables en simultáneo. Probamos la performance de las redes utilizando
distintas técnicas de entrenamiento: estocástico, batch y mini batch con distintos tamaños. Al mismo tiempo, variamos 
la distribución utilizada para la inicialización de los pesos de la red. Utilizamos distribución normal y uniforme.\\

A la hora de elegir tamaños de batch para las ejecuciones mini batch, elegimos los siguientes tamaños: 5,10,100, mitad del dataset y dataset completo (batch). La idea es probar con batchs pequeños como 5 y 10, cercanos en teoría al online learning y también con algunos tamaños más grandes. Probamos también utilizando el dataset completo.\\

Al igual que en los primeros dos experimentos, juzgaremos las modalidades midiendo la performance final de error sobre dataset de training, error sobre el de validación y efectividad de aciertos prediciendo los patrones de testing.\\
Observemos los resultados:

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/distribucion_error_final.png}
  \caption{Error final (función de costo) promedio para cada cantidad de neuronas utilizando el dataset de entrenamiento}
  \end{center}
\end{figure}

En la primera figura de este experimento mostramos la función de costo obtenida con las diferentes técnicas.

Notación:
\begin{itemize}
\item $B1$ = Entrenamiento estocástico.
\item $BX$ = Mini batch de tamaño $X$.
\item $B Mitad$ = Mini batch de tamaño equivalente a la mitad del dataset.
\item $B Entero$ = Batch común.
\end{itemize}

A su vez, mostramos en color azul los resultados obtenidos utilizando distribución de pesos uniforme y en naranja, distribución normal.\\
Podemos ver que, tanto para aprendizaje estocástico como para los mini batchs más pequeños, se obtuvieron mejores resultados utilizando la distribución normal.

Para el error final en los datos de validación, obtuvimos los siguientes resultados:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/distribucion_error_validacion.png}
  \caption{Error (función de costo) promedio para cada cantidad de neuronas utilizando el dataset de validación}
  \end{center}
\end{figure}

% En la figura 5 podemos notar que el valor de error con el dataset de validación también decrece a medida que sube la cantidad
% de neuronas, aunque no de forma uniforme. Podemos apreciar que a partir de las 10 neuronas, el error se mantiene por debajo de 0.15.\\

% Para la efectividad en la predicción de los datos de testing, observamos:\\

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.75]{graficos/distribucion_eficiencia.png}
  \caption{Tasa de predicciones correctas, en porcentaje, para el dataset de testing}
  \end{center}
\end{figure}

% Nuevamente podemos apreciar en la figura 6, que a medida que crece la cantidad de neuronas, crece la cantidad de aciertos sobre el conjunto de datos de test. Sin embargo, los resultados se comienzan a estancar a partir de las 10 neuronas en valores cercanos al 90 \% de aciertos.\\

% Observando los resultados obtenidos en las pruebas, decidimos que la mejor configuración es con 10 neuronas por capa. Esta decisión se justifica teniendo en cuenta que los resultados no mejoran demasiado pasando de 10 y si empeora notablemente la velocidad de la red.\\
% Por ende, \textbf{en vistas a los buenos resultados obtenidos con 10 neuronas y no obteniendo mejoras sustanciales con cantidades mayores}, nos quedaremos con este valor.

\subsubsection{Performance de la red, sin y con preprocesamiento de los patrones.}

\subsubsection{Performance de la red, sin y con early-stopping.}

\subsubsection{Performance de la red, variando las funciones de activación y/o sus parámetros.}


\subsection{Descripción, justificación y performance, de la solución óptima propuesta.}


\subsection{Conclusiones.}